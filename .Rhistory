xref = 'paper',
yref = 'paper',
showarrow = TRUE
))
View(mtcars)
View(mtcars)
str(volcano)
View(volcano)
#the 3D surface map
plot_ly(z = ~volcano, type = "surface")
install.packages("corrplot")
library(corrplot)
#check if all the variables arew numeric
str(mtcars)
#compute the correlation matrix of these variables
coorMat = cor(mtcars)
library(corrplot)
#check if all the variables arew numeric
str(mtcars)
#compute the correlation matrix of these variables
coorMat = cor(mtcars)
library(corrplot)
#compute the correlation matrix of these variables
corrMat = cor(mtcars)
#Correlations
install.packages("corrplot")
library(corrplot)
#check if all the variables arew numeric
str(mtcars)
#compute the correlation matrix of these variables
corrMat = cor(mtcars)
#Generate the correlation plot
corrplot(corrMat)
#Generate the correlation ellipse plot
corrplot(corrMat, method = "ellipse")
library(corrplot)
#check if all the variables arew numeric
str(mtcars)
#compute the correlation matrix of these variables
corrMat = cor(mtcars)
#compute the correlation matrix of these variables
corrMat = corr(mtcars)
# missing value Inputation - embarked charachter values
table(train$Embarked, useNA = "always") #mode = S
sum(is.na(i1$Sepal.Length)==TRUE)/length(i1$Sepal.Length)
train <-  read.csv2("train.csv", header = TRUE, sep = ",")
View(train)
sapply(train, function(train){
sum(is.na(train)==T)/length(train)
})
sapply(train, function(x) sum(x==""))
sum(is.na(train$Age)==TRUE)/length(train$Age)
#substitute the missing value with the average value
train[is.na(train$Age)] <- mean(train$Age, na.rm = TRUE)
sum(is.na(train$Age))
# missing value Inputation - embarked charachter values
table(train$Embarked, useNA = "always") #mode = S
#missing value Inputation - Fare
#substitut the missing value with the average value
train$fFare[is.na(train$Fare)] <- mean(train$Fare, na.rm = T)
sum(is.na(train$Fare))
sapply(train, function(df){
sum(is.na(df)==T)/length(df)
})
########################data exploration | Identify missings values##############
df
######################importing text files###########
sp <-  read.csv2("shopp.csv", header = TRUE, sep = ",")
View(sp)
######################importing text files###########
sp <-  read.csv2("shopp.csv", header = TRUE, sep = ",")
View(sp)
######################importing text files###########
sp <-  read.csv2("shopp.csv", header = TRUE, sep = ",")
View(sp)
######################importing text files###########
sp <-  read.csv2("shopp.csv", header = TRUE, sep = ",")
View(sp)
install.packages("e1071")
#importing the dataset
dataset <- read.csv("Social_Network_Ads.csv")
dataset = dataset[3:5]
# Encoding the target feature as a factor
dataset$Purchased = factor(dataset$Purchased, levels = c(0, 1))
View(dataset)
print(read.csv("Social_Network_Ads.csv"))
# Encoding the target feature as a factor
dataset$Purchased = factor(dataset$Purchased, levels = c(0, 1))
# splitting the dataset into a training set and Test set
install.packages("caTools")
set.seed(123)
# splitting the dataset into a training set and Test set
install.packages("caTools")
library(caTools)
set.seed(123)
split = sample.split(dataset$Purchased, splitRatio = 0.75)
# splitting the dataset into a training set and Test set
# install.packages("caTools")
library(caTools)
set.seed(123)
######################importing text files###########
sp <-  read.csv2("shopp.csv", header = TRUE, sep = ",")
######################importing text files###########
sp <-  read.csv2("shopp.csv", header = TRUE, sep = ",")
View(sp)
#importing the dataset
dataset <- read.csv("Social_Network_Ads.csv")
dataset = dataset[3:5]
# Encoding the target feature as a factor
dataset$Purchased = factor(dataset$Purchased, levels = c(0, 1))
# splitting the dataset into a training set and Test set
# install.packages("caTools")
library(caTools)
set.seed(123)
split = sample.split(dataset$Purchased, splitRatio = 0.75)
split = sample.split(dataset$Purchased, SplitRatio = 0.75)
training_set = subset(dataset, split = TRUE)
test_set = subset(dataset, split=FALSE)
split
training_set
test_set
View(test_set)
split = Sample.split(dataset$Purchased, SplitRatio = 0.75)
split = sample.split(dataset$Purchased, SplitRatio = 0.75)
training_set = subset(dataset, split == TRUE)
test_set = subset(dataset, split == FALSE)
# feature scaling
training_set[-3] = scale(training_set[-3])
test_set[-3] = scale(test_set[-3])
View(training_set)
# Fitting SVM to the training set
# install.packages("e1071")
library(e1071)
classifier = naiveBayes(x = training_set[-3],
y = training_set$Purchased)
View(classifier)
View(classifier)
# Predict the Test set result
y_pred = predict(classifier, newdata = test_set[-3])
View(y_pred)
View(test_set)
View(test_set[, 3])
# making the confusion matrix
library(caret)
# making the confusion matrix
cm = table(test_set[,3], y_pred)
cm
View(cm)
confusionMatrix(cm)
cm
View(cm)
# visualizing the training set result ~ use it as a template
library(ElemStatLearn)
install.packages("ElemStatLearn")
set = training_set
x1 = seq(min(set[,1])-1, max(set[,1]) + 1, by = 0.01)
x2 = seq(min(set[,2])-1, max(set[,2]) + 1, by = 0.01)
grid_set = expand.grid(x1, x2)
colnames(grid_set) = c('Age', 'EstimatedSalary')
y_grid = predict(classifier, newdata = grid_set)
plot(set[,3],
main = 'Native bayes (Training ser)',
xlab = 'Age', ylab = 'EstimatedSalary',
xlim = range(x1), ylim = range(x2))
contour(x1,x2, matrix(as.numeric(y_grid), length(x1), length(x2)), add = TRUE)
points(grid_set, pch = '.', col = ifelse(y_grid == 1, 'springgreen3', 'tomato'))
points(grid_set, pch = '.', col = ifelse(set[,3] == 1, 'green4', 'red3'))
plot(set[,3],
main = 'Native bayes (Training set)',
xlab = 'Age', ylab = 'EstimatedSalary',
xlim = range(x1), ylim = range(x2))
plot(set[,3],
main = 'Native bayes (Training set)',
xlab = 'Age', ylab = 'EstimatedSalary',
xlim = range(x1), ylim = range(x2))
# visualizing the training set result ~ use it as a template
library(ElemStatLearn)
set = training_set
x1 = seq(min(set[,1])-1, max(set[,1]) + 1, by = 0.01)
x2 = seq(min(set[,2])-1, max(set[,2]) + 1, by = 0.01)
grid_set = expand.grid(x1, x2)
colnames(grid_set) = c('Age', 'EstimatedSalary')
y_grid = predict(classifier, newdata = grid_set)
plot(set[,3],
main = 'Native bayes (Training set)',
xlab = 'Age', ylab = 'EstimatedSalary',
xlim = range(x1), ylim = range(x2))
contour(x1,x2, matrix(as.numeric(y_grid), length(x1), length(x2)), add = TRUE)
points(grid_set, pch = '.', col = ifelse(y_grid == 1, 'springgreen3', 'tomato'))
points(grid_set, pch = '.', col = ifelse(set[,3] == 1, 'green4', 'red3'))
points(grid_set, pch = 21, bg = ifelse(set[,3] == 1, 'green4', 'red3'))
# visualizing the test set results
library(ElemStatLearn)
set = test_set
x1 = seq(min(set[,1])-1, max(set[,1]) + 1, by = 0.01)
x2 = seq(min(set[,2])-1, max(set[,2]) + 1, by = 0.01)
grid_set = expand.grid(x1, x2)
colnames(grid_set) = c('Age', 'EstimatedSalary')
y_grid = predict(classifier, newdata = grid_set)
plot(set[,3],
main = 'Native bayes (test set)',
xlab = 'Age', ylab = 'EstimatedSalary',
xlim = range(x1), ylim = range(x2))
contour(x1,x2, matrix(as.numeric(y_grid), length(x1), length(x2)), add = TRUE)
points(grid_set, pch = '.', col = ifelse(y_grid == 1, 'springgreen3', 'tomato'))
points(grid_set, pch = '.', col = ifelse(set[,3] == 1, 'green4', 'red3'))
# visualizing the test set results
library(ElemStatLearn)
set = test_set
x1 = seq(min(set[,1])-1, max(set[,1]) + 1, by = 0.01)
x2 = seq(min(set[,2])-1, max(set[,2]) + 1, by = 0.01)
grid_set = expand.grid(x1, x2)
colnames(grid_set) = c('Age', 'EstimatedSalary')
y_grid = predict(classifier, newdata = grid_set)
plot(set[,3],
main = 'Native bayes (test set)',
xlab = 'Age', ylab = 'EstimatedSalary',
xlim = range(x1), ylim = range(x2))
contour(x1,x2, matrix(as.numeric(y_grid), length(x1), length(x2)), add = TRUE)
points(grid_set, pch = '.', col = ifelse(y_grid == 1, 'springgreen3', 'tomato'))
points(grid_set, pch = 21, bg = ifelse(set[,3] == 1, 'green4', 'red3'))
# visualizing the test set results
library(ElemStatLearn)
set = test_set
x1 = seq(min(set[,1])-1, max(set[,1]) + 1, by = 0.01)
x2 = seq(min(set[,2])-1, max(set[,2]) + 1, by = 0.01)
grid_set = expand.grid(x1, x2)
colnames(grid_set) = c('Age', 'EstimatedSalary')
y_grid = predict(classifier, newdata = grid_set)
plot(set[,3],
main = 'Native bayes (test set)',
xlab = 'Age', ylab = 'EstimatedSalary',
xlim = range(x1), ylim = range(x2))
contour(x1,x2, matrix(as.numeric(y_grid), length(x1), length(x2)), add = TRUE)
points(grid_set, pch = '.', col = ifelse(y_grid == 1, 'springgreen3', 'tomato'))
points(grid_set, pch = 21, bg = ifelse(set[,3] == 1, 'green4', 'red3'))
# visualizing the test set results
library(ElemStatLearn)
set = test_set
x1 = seq(min(set[,1])-1, max(set[,1]) + 1, by = 0.01)
x2 = seq(min(set[,2])-1, max(set[,2]) + 1, by = 0.01)
grid_set = expand.grid(x1, x2)
colnames(grid_set) = c('Age', 'EstimatedSalary')
y_grid = predict(classifier, newdata = grid_set)
View(set)
View(dataset)
View(good_volume)
View(test_set)
View(training_set)
View(train)
#importing the dataset
dataset <- read.csv("Social_Network_Ads.csv")
View(dataset)
cm
confusionMatrix(cm)
# evaluating the model
library(caret)
confusionMatrix(cm)
# visualizing the training set result ~ use it as a template
library(ElemStatLearn)
set = training_set
x1 = seq(min(set[,1])-1, max(set[,1]) + 1, by = 0.01)
x2 = seq(min(set[,2])-1, max(set[,2]) + 1, by = 0.01)
grid_set = expand.grid(x1, x2)
colnames(grid_set) = c('Age', 'EstimatedSalary')
y_grid = predict(classifier, newdata = grid_set)
plot(set[,3],
main = 'Native bayes (Training set)',
xlab = 'Age', ylab = 'EstimatedSalary',
xlim = range(x1), ylim = range(x2))
contour(x1,x2, matrix(as.numeric(y_grid), length(x1), length(x2)), add = TRUE)
points(grid_set, pch = '.', col = ifelse(y_grid == 1, 'springgreen3', 'tomato'))
points(grid_set, pch = 21, bg = ifelse(set[,3] == 1, 'green4', 'red3'))
install.packages("rpart")
View(mtcars)
View(mtcars)
install.packages("rpart.plot")
############################DECISION TREE#################
#Exaple 1: mtcars
#decision tree
data(mtcars)
str(mtcars)
mtcars$vs <- as.factor(mtcars$vs)
mtcars$am <- as.factor(mtcars$am)
#using rpart function of rpart package
#step 1: Split data in train and test data
library(caTools)
set.seed(123)
split<-sample.split(mtcars, SplitRatio = 0.8)
split<-sample.split(mtcars, SplitRatio = 0.8)
split
train <- subset(mtcars, split == "TRUE")
test  <- subset(mtcars, split == "FALSE")
str(train)
str(test)
############################DECISION TREE#################
#Exaple 1: mtcars
#decision tree
data(mtcars)
str(mtcars)
mtcars$vs <- as.factor(mtcars$vs)
mtcars$am <- as.factor(mtcars$am)
#using rpart function of rpart package
#step 1: Split data in train and test data
library(caTools)
set.seed(123)
split<-sample.split(mtcars, SplitRatio = 0.8)
split
train <- subset(mtcars, split == "TRUE")
test  <- subset(mtcars, split == "FALSE")
str(train)
str(test)
library(rpart, lib.loc = "/usr/lib/R/library")
remove.packages("rpart", lib="/usr/lib/R/library")
remove.packages("rpart", lib="/usr/lib/R/library")
DecTreeModel <- rpart(am~. , data = train, method = "class")
View(DecTreeModel)
rpart.plot(DecTreeModel)
#see the decision tree
#install("rpart.plot")
library("rpart.plot")
rpart.plot(DecTreeModel)
#Step 3: predict on test data
fitted.value <- predict(DecTreeModel, newdata = test, type = "class")
fitted.value
#Step 3: predict on test data
fitted <- predict(DecTreeModel, newdata = test, type = "class")
fitted
#Step 3: predict on test data
fitted.value <- predict(DecTreeModel, newdata = test, type = "class")
fitted.value
test$am
#step 4: Evaluate the model accuracy
table(test$am, fitted.value)
misClassError <- mean(fitted.value != test$am)
print(paste('accuracy = ', 1-misClassEroor))
print(paste('accuracy = ', 1-misClassError))
library(rpart)
library(rpart.plot)
library(e1071)
head(iris)
iris
decision_tree_model <- rpart(Species ~. , data = iris, method = "class")
decision_tree_model
rpart.plot(decision_tree_model)
# running rthe model in test and train
library(caTools)
set.seed(123)
split <- sample.split(iris, SplitRatio = 0.7)
split
train = subset(iris, split = "TRUE")
test <- subset(iris, split == "FALSE")
train
test
#train the model on train data
decision_tree_model = rpart(Species ~., data = train, method = "class")
summary(decision_tree_model)
#The princp plotcp functions provide the cross-validation error for each nsplit
#the one with leat crossvalidated error (xerror) is optimal value of cpgiven
#minimum error occur when CP = 0.01
printcp(decision_tree_model)
?printcp
#minimum error occurs the tree size is = 3
plotcp(decision_tree_model)
###################################################################################################
#Example 3: chunrn data
###################################################################################################
#import churm data
churn_data = read.csv(file.choose())
str(churn_data)
#step 1: split data ain train and test data
library(caTools)
set.seed(350)
split<-sample.split(churn_data, SplitRatio = 0.7)
split
train <- subset(churn_data, split == "TRUE")
test <- subset(churn_data, split=="FALSE")
str(train)
str(test)
#step2 : Train model logistics regression using glm function
decision_tree_model2 = rpart(churn_data, ., data = train, method = "class")
View(churn_data)
#step2 : Train model logistics regression using glm function
decision_tree_model2 = rpart(talk_time~ ., data = train, method = "class")
decision_tree_model2
summary(decision_tree_model2)
printcp(decision_tree_model2)
plotcp(decision_tree_model2)
rpart.plot(decision_tree_model2)
str(test)
#step2 : Train model logistics regression using glm function
decision_tree_model2 = rpart(n_cores~ ., data = train, method = "class")
decision_tree_model2
summary(decision_tree_model2)
printcp(decision_tree_model2)
plotcp(decision_tree_model2)
rpart.plot(decision_tree_model2)
#predict test data based on trained model
test$n_cores_predicted = predict(decision_tree_model2, newdata = test, type = "class")
#Step4 : Evaluate model accuracy using confusion matrix
table(test$n_cores, test$n_cores_predicted)
confusionMatrix(table(test$n_cores, test$n_cores_predicted))
library(caret)
confusionMatrix(table(test$n_cores, test$n_cores_predicted))
plotcp(decision_tree_model2)
#tree Running
#find the value of CP for which cross validation is minute
min(decision_tree_model$cptable[, "xerror"])
which.min(decision_tree_model2$cptable[,"xerror"])
cpmin <- decision_tree_model$cptable[3, "cp"]
cpmin <- decision_tree_model$cptable[3, "CP"]
plotcp(decision_tree_model2)
#tree Prunning
#find the value of CP for which cross validation is minute
min(decision_tree_model$cptable[, "xerror"])
which.min(decision_tree_model2$cptable[,"xerror"])
cpmin <- decision_tree_model$cptable[3, "CP"]
#prune the tree by setting the cp parameter as = cpmin
decision_tree_pruned = prune(decision_tree_model2, cp = cpmin)
rpart.plot(decision_tree_pruned)
printcp(decision_tree_pruned)
plotcp(decision_tree_pruned)
plotcp(decision_tree_pruned)
rpart.plot(decision_tree_pruned)
which.min(decision_tree_model2$cptable[,"xerror"])
#tree Prunning
#find the value of CP for which cross validation is minute
min(decision_tree_model2$cptable[, "xerror"])
which.min(decision_tree_model2$cptable[,"xerror"])
cpmin <- decision_tree_model2$cptable[3, "CP"]
#prune the tree by setting the cp parameter as = cpmin
decision_tree_pruned = prune(decision_tree_model2, cp = cpmin)
rpart.plot(decision_tree_pruned)
printcp(decision_tree_pruned)
plotcp(decision_tree_pruned)
cpmin <- decision_tree_model2$cptable[1, "CP"]
#prune the tree by setting the cp parameter as = cpmin
decision_tree_pruned = prune(decision_tree_model2, cp = cpmin)
rpart.plot(decision_tree_pruned)
printcp(decision_tree_pruned)
plotcp(decision_tree_pruned)
3
3
3
3
3
3
3
cpmin <- decision_tree_model2$cptable[3, "CP"]
#prune the tree by setting the cp parameter as = cpmin
decision_tree_pruned = prune(decision_tree_model2, cp = cpmin)
rpart.plot(decision_tree_pruned)
printcp(decision_tree_pruned)
plotcp(decision_tree_pruned)
plotcp(decision_tree_pruned)
cpmin <- decision_tree_model2$cptable[4, "CP"]
#prune the tree by setting the cp parameter as = cpmin
decision_tree_pruned = prune(decision_tree_model2, cp = cpmin)
rpart.plot(decision_tree_pruned)
printcp(decision_tree_pruned)
plotcp(decision_tree_pruned)
plotcp(decision_tree_pruned)
cpmin <- decision_tree_model2$cptable[2, "CP"]
#prune the tree by setting the cp parameter as = cpmin
decision_tree_pruned = prune(decision_tree_model2, cp = cpmin)
rpart.plot(decision_tree_pruned)
printcp(decision_tree_pruned)
plotcp(decision_tree_pruned)
plotcp(decision_tree_pruned)
decision_tree_model2$cptable[2, "CP"]
rpart.plot(decision_tree_pruned)
#predict test data based on trained model
test$n_cores_predicted = predict(decision_tree_pruned, newdata = test, type = "class")
#Prune the tree by setting the CP parameter as = CPmin
decision_tree_pruned = prune(decision_tree_model2, cp = cpmin)
confusionMatrix(table(test$n_cores, test$n_cores_predicted))
#step 2: fitting Random Forest Classification to the training set
install.packages('randomForest')
#step 2: fitting Random Forest Classification to the training set
install.packages('randomForest')
# Random Forest on Iris Dataset
data("iris")
str(iris)
#step 1: split data in train and test
split <- sample.split(iris, SplitRatio = 0.7)
split
training_set = subset(iris, split == "TRUE")
test_set <- subset(iris, split == "FALSE")
#step 2: fitting Random Forest Classification to the training set
#install.packages('randomForest')
library(randomForest)
set.seed(123)
View(iris)
?randomForest
#step 2: fitting Random Forest Classification to the training set
#install.packages('randomForest')
library(randomForest)
set.seed(123)
?randomForest
classifier = randomForest(x = training_set[-5],
y = training_set$Species,
ntree = 500)
#Step3: predict the test results
y_pred = predict(classifier, newdata = test_set[-5])
#making the confusion matrix
cm = table(test_set[, 5], y_pred)
cm
summary(test_set)
#step4: model evaluation
plot(classifier)
importance(classifier)
varImpPlot(classifier)
